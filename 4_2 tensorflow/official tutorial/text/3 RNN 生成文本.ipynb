{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\tensorflow2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载莎士比亚数据集\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-dupilicate charactor\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handle the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorlize text: two table: one for charactor to number, other for number to charactor\n",
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict task\n",
    "given a charactor or word to predict the next word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将文本拆分为长度为 seq_length+1 的文本块。例如，假设 seq_length 为 4 而且文本为 “Hello”， 那么输入序列将为 “Hell”，目标序列将为 “ello”。\n",
    "\n",
    "为此，首先使用 tf.data.Dataset.from_tensor_slices 函数把文本向量转换为字符索引流。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# set the max length\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)  // seq_length\n",
    "\n",
    "# target\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])  # tensor into numpy to char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(47, shape=(), dtype=int32)\n",
      "tf.Tensor(56, shape=(), dtype=int32)\n",
      "tf.Tensor(57, shape=(), dtype=int32)\n",
      "tf.Tensor(58, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i in char_dataset.take(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch 方法使我们能轻松把单个字符转换为所需长度的序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)  # whole text\n",
    "\n",
    "for item in sequences.take(5):  # every time take 5\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每个序列，使用 map 方法先复制再顺移，以创建输入文本和目标文本。map 方法可以将一个简单的函数应用到每一个批次 （batch）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# print 1st batch and target\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些向量的每个索引均作为一个时间步来处理。作为时间步 0 的输入，模型接收到 “F” 的索引，并尝试预测 “i” 的索引为下一个字符。在下一个时间步，模型执行相同的操作，但是 RNN 不仅考虑当前的输入字符，还会考虑上一步的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step    1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step    2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step    3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step    4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面我们使用 tf.data 将文本拆分为可管理的序列。但是在把这些数据输送至模型之前，我们需要将数据重新排列 （shuffle） 并打包为批次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 批大小\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 设定缓冲区大小，以重新排列数据集\n",
    "# （TF 数据被设计为可以处理可能是无限的序列，\n",
    "# 所以它不会试图在内存中重新排列整个序列。相反，\n",
    "# 它维持一个缓冲区，在缓冲区重新排列元素。） \n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# buidl model\n",
    "使用 tf.keras.Sequential 定义模型。在这个简单的例子中，我们使用了三个层来定义模型：\n",
    "\n",
    "tf.keras.layers.Embedding：输入层。一个可训练的对照表，它会将每个字符的数字映射到一个 embedding_dim 维度的向量。\n",
    "\n",
    "tf.keras.layers.GRU：一种 RNN 类型，其大小由 units=rnn_units 指定（这里你也可以使用一个 LSTM 层）。\n",
    "\n",
    "tf.keras.layers.Dense：输出层，带有 vocab_size 个输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# \n",
    "embedding_dim = 256\n",
    "\n",
    "# RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function standard_gru at 0x000002AA754235E8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000002AA754235E8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_gru at 0x000002AA754235E8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000002AA754235E8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_gru at 0x000002AA75447318> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000002AA75447318>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_gru at 0x000002AA75447318> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000002AA75447318>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](https://github.com/littlebeanbean7/docs/blob/master/site/en/tutorials/text/images/text_generation_training.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查点保存至的目录\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# 检查点的文件名\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 20s 116ms/step - loss: 2.6619\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 18s 104ms/step - loss: 1.9623\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 18s 105ms/step - loss: 1.6965\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 18s 106ms/step - loss: 1.5448\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 1.4531\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 18s 102ms/step - loss: 1.3895\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 18s 102ms/step - loss: 1.3396\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 1.2954\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 18s 102ms/step - loss: 1.2535\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 1.2126\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=10\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恢复最新的检查点\n",
    "为保持此次预测步骤简单，将批大小设定为 1。\n",
    "\n",
    "由于 RNN 状态从时间步传递到时间步的方式，模型建立好之后只接受固定的批大小。\n",
    "\n",
    "若要使用不同的 batch_size 来运行模型，我们需要重建模型并从检查点中恢复权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_10'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function standard_gru at 0x000002AA754235E8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000002AA754235E8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_gru at 0x000002AA754235E8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000002AA754235E8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_gru at 0x000002AA75447318> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000002AA75447318>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_gru at 0x000002AA75447318> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000002AA75447318>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](https://github.com/littlebeanbean7/docs/blob/master/site/en/tutorials/text/images/text_generation_sampling.png?raw=1)\n",
    "预测循环\n",
    "下面的代码块生成文本：\n",
    "\n",
    "首先设置起始字符串，初始化 RNN 状态并设置要生成的字符个数。\n",
    "\n",
    "用起始字符串和 RNN 状态，获取下一个字符的预测分布。\n",
    "\n",
    "然后，用分类分布计算预测字符的索引。把这个预测字符当作模型的下一个输入。\n",
    "\n",
    "模型返回的 RNN 状态被输送回模型。现在，模型有更多上下文可以学习，而非只有一个字符。在预测出下一个字符后，更改过的 RNN 状态被再次输送回模型。模型就是这样，通过不断从前面预测的字符获得更多上下文，进行学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # 评估步骤（用学习过的模型生成文本）\n",
    "\n",
    "  # 要生成的字符个数\n",
    "  num_generate = 1000\n",
    "\n",
    "  # 将起始字符串转换为数字（向量化）\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # 空字符串用于存储结果\n",
    "  text_generated = []\n",
    "\n",
    "  # 低温度会生成更可预测的文本\n",
    "  # 较高温度会生成更令人惊讶的文本\n",
    "  # 可以通过试验以找到最好的设定\n",
    "  temperature = 1.0\n",
    "\n",
    "  # 这里批大小为 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # 删除批次的维度\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # 用分类分布预测模型返回的字符\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # 把预测字符和前面的隐藏状态一起传递给模型作为下一个输入\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: have there's nothin me,'s nose,\n",
      "Who against him like a blood, that I mean to bid\n",
      "Will thou there to do himself as leave us?\n",
      "\n",
      "ROMEO:\n",
      "Thou see'st thou go'st not mine Sinful Henry means to\n",
      "ever fooling my body's not scarcely well eonle goes thie;\n",
      "For in your clos what flatter the inform\n",
      "And that his craves with night, and what not your high-seliverace great merrow\n",
      "Or stir on my how here stoppices, who nature had some cto\n",
      "consideth this him\n",
      "To entre thee away: yew to do some time.\n",
      "\n",
      "PETRUCHIO:\n",
      "Came Kent? which should thou we pass'd his thrifts it.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "how let him speak foolmaster;\n",
      "His inscusempter man, thou now, thou met agree.\n",
      "Now for your swoting Boning by their trual drop.\n",
      "\n",
      "RIVERS:\n",
      "When ill, you need not do?\n",
      "If not, my comfort, as sear of death of imprisonment:\n",
      "O bl she smoked; who matter words,\n",
      "'Gond hirdwells needs must up for as there never grown\n",
      "To beauta's witch as if he had but a joyful lust, God is not taked together;\n",
      "You as far as the\n",
      "lamentations, a fair upon this s\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若想改进结果，最简单的方式是延长训练时间 （试试 EPOCHS=30）。\n",
    "\n",
    "你还可以试验使用不同的起始字符串，或者尝试增加另一个 RNN 层以提高模型的准确率，亦或调整温度参数以生成更多或者更少的随机预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = generate_text(model, start_string=u\"I say: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I say: now much is the weart-lame to\\nA scudy, our mindness, be to do it.\\nHoe! tell him his heavy days to ere I amone them gail for honest\\nCliff frights me in our business as his son\\nSecrive no great all-men'd you, to say the tricker's wife.\\n\\nGLOUCESTER:\\nThat Even re-st, manquels to't in boy;\\nWhich is a hely instruction for your friends: thou hast chatest winder child Bupanur.\\n\\nHASTINGS:\\nI know not, when I would they call you do.\\n\\nSICANDIA:\\nAt thy palm thy tribunes are you ours.\\n\\nLUCIO:\\nAy, my lord, the law's not, question for, our fatherly\\nPast her fortune be not to be cathing stain'd topentise\\nWith old han cheap his worthip meetual, if you are not do I ended them to die to-night.\\n\\nJOHN SOLINGBROKE:\\nCome, honest thou not, away;\\nThe mother would have braved death is read it.\\n\\nHASTINGS:\\nAs I with aloof, will I bear me off, and to leave his body,\\nYour mother, and ingrave thee in my true,\\nA leaver am I rumber what thou should ask, to give me to yourself\\nFor turnon me, then damangs put to death!\\nH\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的训练步骤简单，但是能控制的地方不多。\n",
    "\n",
    "至此，你已经知道如何手动运行模型。现在，让我们打开训练循环，并自己实现它。这是一些任务的起点，例如实现 课程学习 以帮助稳定模型的开环输出。\n",
    "\n",
    "你将使用 tf.GradientTape 跟踪梯度。关于此方法的更多信息请参阅 eager execution 指南。\n",
    "\n",
    "步骤如下：\n",
    "\n",
    "首先，初始化 RNN 状态，使用 tf.keras.Model.reset_states 方法。\n",
    "\n",
    "然后，迭代数据集（逐批次）并计算每次迭代对应的 预测。\n",
    "\n",
    "打开一个 tf.GradientTape 并计算该上下文时的预测和损失。\n",
    "\n",
    "使用 tf.GradientTape.grads 方法，计算当前模型变量情况下的损失梯度。\n",
    "\n",
    "最后，使用优化器的 tf.train.Optimizer.apply_gradients 方法向下迈出一步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function standard_gru at 0x000002AA754235E8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000002AA754235E8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_gru at 0x000002AA754235E8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000002AA754235E8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_gru at 0x000002AA75447318> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000002AA75447318>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_gru at 0x000002AA75447318> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000002AA75447318>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(inp)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            target, predictions, from_logits=True))\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function train_step at 0x000002AB011BBCA8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train_step at 0x000002AB011BBCA8>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function train_step at 0x000002AB011BBCA8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train_step at 0x000002AB011BBCA8>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function standard_gru at 0x000002AA754235E8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000002AA754235E8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_gru at 0x000002AA754235E8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000002AA754235E8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_gru at 0x000002AA75447318> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000002AA75447318>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_gru at 0x000002AA75447318> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000002AA75447318>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002AB01259678> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002AB01259678>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002AB01259678> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002AB01259678>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function train_step at 0x000002AB011BBCA8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train_step at 0x000002AB011BBCA8>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function train_step at 0x000002AB011BBCA8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train_step at 0x000002AB011BBCA8>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <function standard_gru at 0x000002AA754235E8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000002AA754235E8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_gru at 0x000002AA754235E8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x000002AA754235E8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_gru at 0x000002AA75447318> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000002AA75447318>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_gru at 0x000002AA75447318> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x000002AA75447318>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.173825740814209\n",
      "Epoch 1 Batch 100 Loss 2.3167788982391357\n",
      "Epoch 1 Loss 2.1459\n",
      "Time taken for 1 epoch 18.6064190864563 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.1589548587799072\n",
      "Epoch 2 Batch 100 Loss 1.8869649171829224\n",
      "Epoch 2 Loss 1.7990\n",
      "Time taken for 1 epoch 16.880786895751953 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.7869982719421387\n",
      "Epoch 3 Batch 100 Loss 1.644071340560913\n",
      "Epoch 3 Loss 1.6069\n",
      "Time taken for 1 epoch 17.476303339004517 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.5778863430023193\n",
      "Epoch 4 Batch 100 Loss 1.5097230672836304\n",
      "Epoch 4 Loss 1.4975\n",
      "Time taken for 1 epoch 17.099212169647217 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.462996006011963\n",
      "Epoch 5 Batch 100 Loss 1.428195834159851\n",
      "Epoch 5 Loss 1.4222\n",
      "Time taken for 1 epoch 17.050785779953003 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.3880597352981567\n",
      "Epoch 6 Batch 100 Loss 1.3704915046691895\n",
      "Epoch 6 Loss 1.3626\n",
      "Time taken for 1 epoch 17.047444820404053 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.3303874731063843\n"
     ]
    }
   ],
   "source": [
    "# 训练步骤\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  # 在每个训练周期开始时，初始化隐藏状态\n",
    "  # 隐藏状态最初为 None\n",
    "  hidden = model.reset_states()\n",
    "\n",
    "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "    loss = train_step(inp, target)\n",
    "\n",
    "    if batch_n % 100 == 0:\n",
    "      template = 'Epoch {} Batch {} Loss {}'\n",
    "      print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "  # 每 5 个训练周期，保存（检查点）1 次模型\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# \n",
    "embedding_dim = 256\n",
    "\n",
    "# RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function standard_lstm at 0x00000291CCAA6678> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x00000291CCAA6678>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x00000291CCAA6678> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x00000291CCAA6678>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x00000291CCAA6708> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x00000291CCAA6708>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x00000291CCAA6708> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x00000291CCAA6708>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查点保存至的目录\n",
    "checkpoint_dir = './training_checkpoints_lstm'\n",
    "\n",
    "# 检查点的文件名\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 25s 144ms/step - loss: 2.6411\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 21s 124ms/step - loss: 1.9543\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 1.6960\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.5457\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.4525\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.3861\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.3328\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.2860\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 21s 122ms/step - loss: 1.2421\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 22s 128ms/step - loss: 1.2003\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 21s 124ms/step - loss: 1.1600\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 22s 125ms/step - loss: 1.1219\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 22s 126ms/step - loss: 1.0873\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 22s 125ms/step - loss: 1.0518\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 22s 125ms/step - loss: 1.0139\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 21s 124ms/step - loss: 0.9776\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 21s 123ms/step - loss: 0.9444\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 21s 124ms/step - loss: 0.9154\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 21s 124ms/step - loss: 0.8878\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 22s 126ms/step - loss: 0.8585\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints_lstm\\\\ckpt_20'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成存储点\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function standard_lstm at 0x00000291CCAA6678> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x00000291CCAA6678>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x00000291CCAA6678> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x00000291CCAA6678>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x00000291CCAA6708> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x00000291CCAA6708>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x00000291CCAA6708> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x00000291CCAA6708>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要生成的字符个数\n",
    "num_generate = 1000\n",
    "\n",
    "def generate_text(model, start_string, num_generate):\n",
    "  # 评估步骤（用学习过的模型生成文本）\n",
    "\n",
    "  # 要生成的字符个数\n",
    "  num_generate = num_generate\n",
    "\n",
    "  # 将起始字符串转换为数字（向量化）\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # 空字符串用于存储结果\n",
    "  text_generated = []\n",
    "\n",
    "  # 低温度会生成更可预测的文本\n",
    "  # 较高温度会生成更令人惊讶的文本\n",
    "  # 可以通过试验以找到最好的设定\n",
    "  temperature = 1.0\n",
    "\n",
    "  # 这里批大小为 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # 删除批次的维度\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # 用分类分布预测模型返回的字符\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # 把预测字符和前面的隐藏状态一起传递给模型作为下一个输入\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Dork more noble Duke of Nurce,\n",
      "Shall have good heart, which for some goier eye owr request,\n",
      "Could I, resign to see your house: the jule tears was king,\n",
      "I seem not what you cram, let me too; he tender yeath\n",
      "your love must lie unbaster'd here\n",
      "From what shoutings is so that twou foregother. What torture use is this yone,\n",
      "His rude cliffory rejoice and fair tongue must be,\n",
      "But left thy brother than she ends at it.\n",
      "But fam she hath; for being their courther ridagators!\n",
      "To-morrow Come, he comes have Romeo by such fen\n",
      "Me his tongue but passing: let's to be lost;\n",
      "And with his drafes\n",
      "sitting his chair; the kind goods cleans may be doubted him,\n",
      "Both him and queen and steeling on my life, if my nature\n",
      "We here for give can fall ram the death of life,\n",
      "Canisters from the harvestom, bloody custom, but upon him: it is her\n",
      "The case of honour than banishment,\n",
      "Refenting twos, though it caupper you to the\n",
      "uggery,\n",
      "For make him the bestore unto the entreaties i' the other, and\n",
      "proclaim her horse; uto let no more whose virtues is your place\n",
      "And saying so story of sick heaven rife, my fortune is post to you undoil'd your eyes;\n",
      "Poor manners hath the frush of kings-one, as yours, you\n",
      "close this afternoon;\n",
      "And for these loves me; lead thy seat and guilty cheek,\n",
      "Unless you have it sectly eye another lequest,\n",
      "Come, you were dost thy onte sorrow can do not\n",
      "Rest, as we have on honour undertake't.\n",
      "\n",
      "QUEEN TANGARET:\n",
      "How came it to our tradesy.\n",
      "\n",
      "CORIOLANUS:\n",
      "That should sm!\n",
      "\n",
      "Clowers:\n",
      "These were command,\n",
      "If thou dose that, that they shall hear you wenc:\n",
      "But is the name of safe I for a whore,\n",
      "And grace by proft loath than my tyranny\n",
      "Stake he with such a new dare accep.\n",
      "\n",
      "TEBAntass for you.\n",
      "\n",
      "AUTOLYCUS:\n",
      "How born to stot? Some out your heads that of a tap-house treacherom, sound,\n",
      "Will maiden me what you would have some aching\n",
      "Here is impossible actor of France\n",
      "And stark out three and my iferution you, thou must know'st, you, door! and we willdraw you\n",
      "lest in the holy services, they follow you.\n",
      "\n",
      "KING RICHARD III:\n",
      "R four my remedies, Post less mistress, life, I poor.\n",
      "3 KING HENRY VI\n",
      "\n",
      "CURTIS:\n",
      "I would the true musicians scarch.\n",
      "\n",
      "PAULINA:\n",
      "There is my word: it is a man-tworn sires ye were o'ce\n",
      "Their passion, kisson, pity fight whereon your brother,\n",
      "Begundly by the year or ancustome should remumperance.\n",
      "Scurders! What courses should me, these your court is victooy?\n",
      "We will be subject for our country's business in thy face of loving,\n",
      "The wretch did scrutch into the brain?\n",
      "\n",
      "LUCENTIO:\n",
      "Here comes your fastic guard, my father fair\n",
      "To use it with him!\n",
      "\n",
      "MENENIUS:\n",
      "In gody severen.\n",
      "\n",
      "BRANUT:\n",
      "Be you depite of other like!\n",
      "It was the outs, but not thy brother slave,\n",
      "Shark me with honours and unfair waliquitition,\n",
      "To burn thy tamy to backmprew: how willit how he know thee\n",
      "dangler, here?\n",
      "\n",
      "MENENIUS:\n",
      "In that I would have been in Rome was time,\n",
      "That every master, he does courtedy a most obedient meaning,\n",
      "But to drunk the tremble, but thou speak'st\n",
      "To threefore stands, with strewer, mockly among,\n",
      "So roudy, hearthips, whats?\n",
      "\n",
      "First Citizens:\n",
      "How made you\n",
      "Some dignorition, fair making on our lady a wisdoms throw whose utterious husband's royal roof!\n",
      "\n",
      "SEBASTIAN:\n",
      "And there should meet youe loving cock? I shall else young.\n",
      "\n",
      "EDWARD:\n",
      "Signiou will thine own sixteen ye nor heaven, and Margurether,\n",
      "And were to falce the royalties and robes.\n",
      "\n",
      "MENENIUS:\n",
      "Longer my sorrow's name of land\n",
      "Should he fight dir concervanions, suffer and fled.\n",
      "\n",
      "CaTERLA:\n",
      "By heavy sir, your sword,\n",
      "As whiles:\n",
      "I thank your woeship in his business sound the world,\n",
      "And strew thy bedue sound where you may love the king,\n",
      "Your mistress should by one rubound my further: but the lord of it\n",
      "Is te rust sweet we shall hear it all.\n",
      "\n",
      "FLONR:\n",
      "He case it, forth he but my head i' the warves us to good.\n",
      "\n",
      "LUCENTIO:\n",
      "Well, you see you well?\n",
      "\n",
      "First Cutizen:\n",
      "An indeed, my lord,\n",
      "She mightster, give for Edwardshbrive.\n",
      "\n",
      "LEONTES:\n",
      "To supper, the good prieful king in wrone,\n",
      "The close them, as you carried all now studiest you not known.\n",
      "How for my patricians, my word; it saddest, be cold,\n",
      "But made be moou the noble just and Duke great piceoce,\n",
      "For this our foers, amine.\n",
      "\n",
      "\n",
      "TRANIO:\n",
      "And this priest below my cousin's house to leave your hands,\n",
      "But fortune with your children, sir, I will comm a gardeman not,\n",
      "Should not have hore what else? what then? what you whom now,\n",
      "Besides she come home; I had showed my husband's looks.\n",
      "\n",
      "ANRELO:\n",
      "I seeks here lies; I'll purchase you, I'll surve againqure\n",
      "The Lords of Romens, hours his court which touch'd you, looks to Have arrose, my heirt withar the father\n",
      "That he requires when he rouse me in my leave, by them Angelo;\n",
      "I do keep Julion! how we would have done, if we avous;\n",
      "We must I speak again; let specivities and pity mis\n",
      "solemning Clurcharck and your clook-maid\n",
      "nags, Show they as farewell's what thou commend'st the humour\n",
      "That his shoftle join'd to soon and husband grave,\n",
      "And the duke hurts from the bu3O:\n",
      "Out, af answer is! Kas he wind\n",
      "Why should hope turn to the burious meaning.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "But dost th\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \", num_generate=5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论\n",
    "1. 训练速度：LSTM 比 GNU 慢，原理上来看，GNU是LSTM的简化版本，省略了游戏额参数，计算过程自然要快\n",
    "2. EPOCHS 训练次数增加之后，生成的文字更像原文风格\n",
    "3. 设想中文的训练，生成歌词，古诗文，等等"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
