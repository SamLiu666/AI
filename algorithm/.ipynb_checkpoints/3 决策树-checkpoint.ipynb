{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dt](https://github.com/zotroneneis/machine_learning_basics/raw/c21753daa38c6f321a97942a721751ff7cbedd91/figures/decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CART training algorithm\n",
    " - 基尼系数算法\n",
    " - $J(D,\\Theta) = \\frac{n_{left}}{n_{right}}G_{left} + \\frac{n_{right}}{n_{left}}G_{right}$\n",
    " \n",
    " - $D$: remaining training examples\n",
    " - $n_{total}$ : number of remaining training examples\n",
    " - $\\theta = (f, t_f)$: feature and feature threshold\n",
    " - $n_{left}/n_{right}$: number of samples in the left/right subset\n",
    " - $G_{left}/G_{right}$: Gini impurity of the left/right subset\n",
    " \n",
    "2. Gini Impurity :Given $K$ different classification values $k \\in \\{1, ..., K\\}$ the Gini impurity of node $m$ is computed as follows:\n",
    "\n",
    "$$G_m = 1-\\sum_{k=1}^{K} (p_{m,k})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(123)\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use iris dataset\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.1, 3.5, 1.4, 0.2]), 0, (150, 4), (150,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0],y[0],X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train: (112, 4)\n",
      "Shape y_train: (112,)\n",
      "Shape X_test: (38, 4)\n",
      "Shape y_test: (38,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(f'Shape X_train: {X_train.shape}')\n",
    "print(f'Shape y_train: {y_train.shape}')\n",
    "print(f'Shape X_test: {X_test.shape}')\n",
    "print(f'Shape y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "def split_dataset(X, y, feature_idx, threshold):\n",
    "    \"\"\"\n",
    "    Splits dataset X into two subsets, according to a given feature\n",
    "    and feature threshold.\n",
    "\n",
    "    Args:\n",
    "        X: 2D numpy array with data samples\n",
    "        y: 1D numpy array with labels\n",
    "        feature_idx: int, index of feature used for splitting the data\n",
    "        threshold: float, threshold used for splitting the data\n",
    "\n",
    "    Returns:\n",
    "        splits: dict containing the left and right subsets\n",
    "        and their labels\n",
    "    \"\"\"\n",
    "\n",
    "    left_idx = np.where(X[:, feature_idx] < threshold)\n",
    "    right_idx = np.where(X[:, feature_idx] >= threshold)\n",
    "\n",
    "    left_subset = X[left_idx]\n",
    "    y_left = y[left_idx]\n",
    "\n",
    "    right_subset = X[right_idx]\n",
    "    y_right = y[right_idx]\n",
    "\n",
    "    splits = {\n",
    "    'left': left_subset,\n",
    "    'y_left': y_left,\n",
    "    'right': right_subset,\n",
    "    'y_right': y_right,\n",
    "    }\n",
    "\n",
    "    return splits\n",
    "\n",
    "# cost function\n",
    "def get_cost(splits， n_classes):\n",
    "    \"\"\"\n",
    "    Computes cost of a split given the Gini impurity of\n",
    "    the left and right subset and the sizes of the subsets\n",
    "\n",
    "    Args:\n",
    "        splits: dict, containing params of current split,from split_dataset\n",
    "    \"\"\"\n",
    "    y_left = splits['y_left']\n",
    "    y_right = splits['y_right']\n",
    "\n",
    "    n_left = len(y_left)\n",
    "    n_right = len(y_right)\n",
    "    n_total = n_left + n_right\n",
    "\n",
    "    gini_left, gini_right = gini_impurity(y_left, y_right, n_left, n_right, n_classes)\n",
    "    cost = (n_left / n_total) * gini_left + (n_right / n_total) * gini_right\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gini index\n",
    "def gini_impurity(y_left, y_right, n_left, n_right, n_classes):\n",
    "    \"\"\"\n",
    "    Computes Gini impurity of a split.\n",
    "\n",
    "    Args:\n",
    "        y_left, y_right: target values of samples in left/right subset\n",
    "        n_left, n_right: number of samples in left/right subset\n",
    "\n",
    "    Returns:\n",
    "        gini_left: float, Gini impurity of left subset\n",
    "        gini_right: gloat, Gini impurity of right subset\n",
    "        \n",
    "        n_classes: class of dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    n_total = n_left + n_right  # right+left\n",
    "\n",
    "    score_left, score_right = 0, 0\n",
    "    gini_left, gini_right = 0, 0\n",
    "\n",
    "    if n_left != 0:\n",
    "        for c in range(n_classes):\n",
    "            # For each class c, compute fraction of samples with class c\n",
    "            p_left = len(np.where(y_left == c)[0]) / n_left\n",
    "            score_left += p_left * p_left\n",
    "        gini_left = 1 - score_left\n",
    "\n",
    "    if n_right != 0:\n",
    "        for c in range(n_classes):\n",
    "            p_right = len(np.where(y_right == c)[0]) / n_right\n",
    "            score_right += p_right * p_right\n",
    "        gini_right = 1 - score_right\n",
    "\n",
    "    return gini_left, gini_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "def get_cost(splits):\n",
    "    \"\"\"\n",
    "    Computes cost of a split given the Gini impurity of\n",
    "    the left and right subset and the sizes of the subsets\n",
    "\n",
    "    Args:\n",
    "        splits: dict, containing params of current split\n",
    "    \"\"\"\n",
    "    y_left = splits['y_left']\n",
    "    y_right = splits['y_right']\n",
    "\n",
    "    n_left = len(y_left)\n",
    "    n_right = len(y_right)\n",
    "    n_total = n_left + n_right\n",
    "\n",
    "    gini_left, gini_right = self.gini_impurity(y_left, y_right, n_left, n_right)\n",
    "    cost = (n_left / n_total) * gini_left + (n_right / n_total) * gini_right\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Decision tree for classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root_dict = None\n",
    "        self.tree_dict = None\n",
    "\n",
    "    def split_dataset(self, X, y, feature_idx, threshold):\n",
    "        \"\"\"\n",
    "        Splits dataset X into two subsets, according to a given feature\n",
    "        and feature threshold.\n",
    "\n",
    "        Args:\n",
    "            X: 2D numpy array with data samples\n",
    "            y: 1D numpy array with labels\n",
    "            feature_idx: int, index of feature used for splitting the data\n",
    "            threshold: float, threshold used for splitting the data\n",
    "\n",
    "        Returns:\n",
    "            splits: dict containing the left and right subsets\n",
    "            and their labels\n",
    "        \"\"\"\n",
    "\n",
    "        left_idx = np.where(X[:, feature_idx] < threshold)\n",
    "        right_idx = np.where(X[:, feature_idx] >= threshold)\n",
    "\n",
    "        left_subset = X[left_idx]\n",
    "        y_left = y[left_idx]\n",
    "\n",
    "        right_subset = X[right_idx]\n",
    "        y_right = y[right_idx]\n",
    "\n",
    "        splits = {\n",
    "        'left': left_subset,\n",
    "        'y_left': y_left,\n",
    "        'right': right_subset,\n",
    "        'y_right': y_right,\n",
    "        }\n",
    "\n",
    "        return splits\n",
    "\n",
    "    def gini_impurity(self, y_left, y_right, n_left, n_right):\n",
    "        \"\"\"\n",
    "        Computes Gini impurity of a split.\n",
    "\n",
    "        Args:\n",
    "            y_left, y_right: target values of samples in left/right subset\n",
    "            n_left, n_right: number of samples in left/right subset\n",
    "\n",
    "        Returns:\n",
    "            gini_left: float, Gini impurity of left subset\n",
    "            gini_right: gloat, Gini impurity of right subset\n",
    "        \"\"\"\n",
    "\n",
    "        n_total = n_left + n_left\n",
    "\n",
    "        score_left, score_right = 0, 0\n",
    "        gini_left, gini_right = 0, 0\n",
    "\n",
    "        if n_left != 0:\n",
    "            for c in range(self.n_classes):\n",
    "                # For each class c, compute fraction of samples with class c\n",
    "                p_left = len(np.where(y_left == c)[0]) / n_left\n",
    "                score_left += p_left * p_left\n",
    "            gini_left = 1 - score_left\n",
    "\n",
    "        if n_right != 0:\n",
    "            for c in range(self.n_classes):\n",
    "                p_right = len(np.where(y_right == c)[0]) / n_right\n",
    "                score_right += p_right * p_right\n",
    "            gini_right = 1 - score_right\n",
    "\n",
    "        return gini_left, gini_right\n",
    "\n",
    "    def get_cost(self, splits):\n",
    "        \"\"\"\n",
    "        Computes cost of a split given the Gini impurity of\n",
    "        the left and right subset and the sizes of the subsets\n",
    "        \n",
    "        Args:\n",
    "            splits: dict, containing params of current split\n",
    "        \"\"\"\n",
    "        y_left = splits['y_left']\n",
    "        y_right = splits['y_right']\n",
    "\n",
    "        n_left = len(y_left)\n",
    "        n_right = len(y_right)\n",
    "        n_total = n_left + n_right\n",
    "\n",
    "        gini_left, gini_right = self.gini_impurity(y_left, y_right, n_left, n_right)\n",
    "        cost = (n_left / n_total) * gini_left + (n_right / n_total) * gini_right\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Finds the best feature and feature index to split dataset X into\n",
    "        two groups. Checks every value of every attribute as a candidate\n",
    "        split.\n",
    "\n",
    "        Args:\n",
    "            X: 2D numpy array with data samples\n",
    "            y: 1D numpy array with labels\n",
    "\n",
    "        Returns:\n",
    "            best_split_params: dict, containing parameters of the best split\n",
    "        \"\"\"\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        best_feature_idx, best_threshold, best_cost, best_splits = np.inf, np.inf, np.inf, None\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            for i in range(n_samples):\n",
    "                current_sample = X[i]\n",
    "                threshold = current_sample[feature_idx]\n",
    "                splits = self.split_dataset(X, y, feature_idx, threshold)\n",
    "                cost = self.get_cost(splits)\n",
    "\n",
    "                if cost < best_cost:\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_threshold = threshold\n",
    "                    best_cost = cost\n",
    "                    best_splits = splits\n",
    "\n",
    "        best_split_params = {\n",
    "            'feature_idx': best_feature_idx,\n",
    "            'threshold': best_threshold,\n",
    "            'cost': best_cost,\n",
    "            'left': best_splits['left'],\n",
    "            'y_left': best_splits['y_left'],\n",
    "            'right': best_splits['right'],\n",
    "            'y_right': best_splits['y_right'],\n",
    "        }\n",
    "\n",
    "        return best_split_params\n",
    "\n",
    "\n",
    "    def build_tree(self, node_dict, depth, max_depth, min_samples):\n",
    "        \"\"\"\n",
    "        Builds the decision tree in a recursive fashion.\n",
    "\n",
    "        Args:\n",
    "            node_dict: dict, representing the current node\n",
    "            depth: int, depth of current node in the tree\n",
    "            max_depth: int, maximum allowed tree depth\n",
    "            min_samples: int, minimum number of samples needed to split a node further\n",
    "\n",
    "        Returns:\n",
    "            node_dict: dict, representing the full subtree originating from current node\n",
    "        \"\"\"\n",
    "        left_samples = node_dict['left']\n",
    "        right_samples = node_dict['right']\n",
    "        y_left_samples = node_dict['y_left']\n",
    "        y_right_samples = node_dict['y_right']\n",
    "\n",
    "        if len(y_left_samples) == 0 or len(y_right_samples) == 0:\n",
    "            node_dict[\"left_child\"] = node_dict[\"right_child\"] = self.create_terminal_node(np.append(y_left_samples, y_right_samples))\n",
    "            return None\n",
    "\n",
    "        if depth >= max_depth:\n",
    "            node_dict[\"left_child\"] = self.create_terminal_node(y_left_samples)\n",
    "            node_dict[\"right_child\"] = self.create_terminal_node(y_right_samples)\n",
    "            return None\n",
    "\n",
    "        if len(right_samples) < min_samples:\n",
    "            node_dict[\"right_child\"] = self.create_terminal_node(y_right_samples)\n",
    "        else:\n",
    "            node_dict[\"right_child\"] = self.find_best_split(right_samples, y_right_samples)\n",
    "            self.build_tree(node_dict[\"right_child\"], depth+1, max_depth, min_samples)\n",
    "\n",
    "        if len(left_samples) < min_samples:\n",
    "            node_dict[\"left_child\"] = self.create_terminal_node(y_left_samples)\n",
    "        else:\n",
    "            node_dict[\"left_child\"] = self.find_best_split(left_samples, y_left_samples)\n",
    "            self.build_tree(node_dict[\"left_child\"], depth+1, max_depth, min_samples)\n",
    "\n",
    "        return node_dict\n",
    "\n",
    "    def create_terminal_node(self, y):\n",
    "        \"\"\"\n",
    "        Creates a terminal node.\n",
    "        Given a set of labels the most common label is computed and\n",
    "        set as the classification value of the node.\n",
    "\n",
    "        Args:\n",
    "            y: 1D numpy array with labels\n",
    "        Returns:\n",
    "            classification: int, predicted class\n",
    "        \"\"\"\n",
    "        classification = max(set(y), key=list(y).count)\n",
    "        return classification\n",
    "\n",
    "    def train(self, X, y, max_depth, min_samples):\n",
    "        \"\"\"\n",
    "        Fits decision tree on a given dataset.\n",
    "\n",
    "        Args:\n",
    "            X: 2D numpy array with data samples\n",
    "            y: 1D numpy array with labels\n",
    "            max_depth: int, maximum allowed tree depth\n",
    "            min_samples: int, minimum number of samples needed to split a node further\n",
    "        \"\"\"\n",
    "        self.n_classes = len(set(y))\n",
    "        self.root_dict = self.find_best_split(X, y)\n",
    "        self.tree_dict = self.build_tree(self.root_dict, 1, max_depth, min_samples)\n",
    "\n",
    "    def predict(self, X, node):\n",
    "        \"\"\"\n",
    "        Predicts the class for a given input example X.\n",
    "\n",
    "        Args:\n",
    "            X: 1D numpy array, input example\n",
    "            node: dict, representing trained decision tree\n",
    "\n",
    "        Returns:\n",
    "            prediction: int, predicted class\n",
    "        \"\"\"\n",
    "        feature_idx = node['feature_idx']\n",
    "        threshold = node['threshold']\n",
    "\n",
    "        if X[feature_idx] < threshold:\n",
    "            if isinstance(node['left_child'], (int, np.integer)):\n",
    "                return node['left_child']\n",
    "            else:\n",
    "                prediction = self.predict(X, node['left_child'])\n",
    "        elif X[feature_idx] >= threshold:\n",
    "            if isinstance(node['right_child'], (int, np.integer)):\n",
    "                return node['right_child']\n",
    "            else:\n",
    "                prediction = self.predict(X, node['right_child'])\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTree()\n",
    "tree.train(X_train, y_train, max_depth=2, min_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petal length (cm) < 3.0, cost of split: 0.346\n",
      "  sepal length (cm) < 5.4, cost of split: 0.0\n",
      "    predicted class: setosa\n",
      "    predicted class: setosa\n",
      "  petal width (cm) < 1.8, cost of split: 0.097\n",
      "    predicted class: versicolor\n",
      "    predicted class: virginica\n"
     ]
    }
   ],
   "source": [
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, (int, np.integer)):\n",
    "        print(f\"{depth * '  '}predicted class: {iris.target_names[node]}\")\n",
    "    else:\n",
    "        print(f\"{depth * '  '}{iris.feature_names[node['feature_idx']]} < {node['threshold']}, \"\n",
    "             f\"cost of split: {round(node['cost'], 3)}\")\n",
    "        print_tree(node[\"left_child\"], depth+1)\n",
    "        print_tree(node[\"right_child\"], depth+1)\n",
    "              \n",
    "\n",
    "print_tree(tree.tree_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    result = tree.predict(X_test[i], tree.tree_dict)\n",
    "    all_predictions.append(y_test[i] == result)\n",
    "\n",
    "print(f\"Accuracy on test set: {sum(all_predictions) / len(all_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use sklearn model\n",
    "https://github.com/machinelearningmindset/machine-learning-course/blob/master/docs/source/content/supervised/decisiontrees.rst\n",
    "![tartge](https://github.com/machinelearningmindset/machine-learning-course/raw/master/docs/source/content/supervised/_img/shopping_table.png)\n",
    "\n",
    "This process is repeated until all nodes have the same value as the target result, or splitting adds no value to a prediction. This algorithm has the root node as the best classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gini impurity function\n",
    "![a](https://github.com/machinelearningmindset/machine-learning-course/raw/master/docs/source/content/supervised/_img/Gini_Impurity.png)\n",
    "\n",
    "cost function:\n",
    "![s](https://github.com/machinelearningmindset/machine-learning-course/raw/master/docs/source/content/supervised/_img/Gini_Information_Gain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import itertools\n",
    "import random \n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The possible values for each class \n",
    "classes = {\n",
    "    'supplies': ['low', 'med', 'high'],\n",
    "    'weather':  ['raining', 'cloudy', 'sunny'],\n",
    "    'worked?':  ['yes', 'no']\n",
    "}\n",
    "\n",
    "# Our example data from the documentation\n",
    "data = [\n",
    "    ['low',  'sunny',   'yes'],\n",
    "    ['high', 'sunny',   'yes'],\n",
    "    ['med',  'cloudy',  'yes'],\n",
    "    ['low',  'raining', 'yes'],\n",
    "    ['low',  'cloudy',  'no' ],\n",
    "    ['high', 'sunny',   'no' ],\n",
    "    ['high', 'raining', 'no' ],\n",
    "    ['med',  'cloudy',  'yes'],\n",
    "    ['low',  'raining', 'yes'],\n",
    "    ['low',  'raining', 'no' ],\n",
    "    ['med',  'sunny',   'no' ],\n",
    "    ['high', 'sunny',   'yes']\n",
    "]\n",
    "# Our target variable, whether someone went shopping\n",
    "target = ['yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit learn can't handle categorical data, so form numeric representations of the above data\n",
    "# Categorical data support may be added in the future: https://github.com/scikit-learn/scikit-learn/pull/4899\n",
    "categories = [classes['supplies'], classes['weather'], classes['worked?']]\n",
    "encoder = OneHotEncoder(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OneHotEncoder(categories=[['low', 'med', 'high'],\n",
       "                           ['raining', 'cloudy', 'sunny'], ['yes', 'no']],\n",
       "               drop=None, dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "               sparse=True),\n",
       " <12x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 36 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = encoder.fit_transform(data)\n",
    "encoder, x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form and fit our decision tree to the now-encoded data\n",
    "classifier = DecisionTreeClassifier()\n",
    "tree = classifier.fit(x_data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our decision tree, let's predict some outcomes from random data\n",
    "# This goes through each class and builds a random set of 5 data points\n",
    "prediction_data = []\n",
    "for _ in itertools.repeat(None, 5):\n",
    "    prediction_data.append([\n",
    "        random.choice(classes['supplies']),\n",
    "        random.choice(classes['weather']),\n",
    "        random.choice(classes['worked?'])\n",
    "    ])\n",
    "\n",
    "# Use our tree to predict the outcome of the random values\n",
    "prediction_results = tree.predict(encoder.transform(prediction_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "def format_array(arr):\n",
    "    return \"\".join([\"| {:<10}\".format(item) for item in arr])\n",
    "\n",
    "def print_table(data, results):\n",
    "    line = \"day  \" + format_array(list(classes.keys()) + [\"went shopping?\"])\n",
    "    print(\"-\" * len(line))\n",
    "    print(line)\n",
    "    print(\"-\" * len(line))\n",
    "\n",
    "    for day, row in enumerate(data):\n",
    "        print(\"{:<5}\".format(day + 1) + format_array(row + [results[day]]))\n",
    "    print(\"\")\n",
    "\n",
    "feature_names = (\n",
    "    ['supplies-' + x for x in classes[\"supplies\"]] +\n",
    "    ['weather-' + x for x in classes[\"weather\"]] +\n",
    "    ['worked-' + x for x in classes[\"worked?\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "---------------------------------------------------------\n",
      "day  | supplies  | weather   | worked?   | went shopping?\n",
      "---------------------------------------------------------\n",
      "1    | low       | sunny     | yes       | yes       \n",
      "2    | high      | sunny     | yes       | no        \n",
      "3    | med       | cloudy    | yes       | no        \n",
      "4    | low       | raining   | yes       | no        \n",
      "5    | low       | cloudy    | no        | yes       \n",
      "6    | high      | sunny     | no        | no        \n",
      "7    | high      | raining   | no        | no        \n",
      "8    | med       | cloudy    | yes       | no        \n",
      "9    | low       | raining   | yes       | no        \n",
      "10   | low       | raining   | no        | yes       \n",
      "11   | med       | sunny     | no        | yes       \n",
      "12   | high      | sunny     | yes       | no        \n",
      "\n",
      "Predicted Random Results:\n",
      "---------------------------------------------------------\n",
      "day  | supplies  | weather   | worked?   | went shopping?\n",
      "---------------------------------------------------------\n",
      "1    | med       | cloudy    | no        | yes       \n",
      "2    | high      | sunny     | no        | no        \n",
      "3    | high      | raining   | no        | no        \n",
      "4    | med       | cloudy    | yes       | no        \n",
      "5    | high      | sunny     | yes       | no        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shows a visualization of the decision tree using graphviz\n",
    "# Note that sklearn is unable to generate non-binary trees, so these are based on individual options in each class\n",
    "dot_data = export_graphviz(tree, filled=True, proportion=True, feature_names=feature_names) \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(filename='decision_tree', cleanup=True, view=True)\n",
    "\n",
    "# Display out training and prediction data and results\n",
    "print(\"Training Data:\")\n",
    "print_table(data, target)\n",
    "\n",
    "print(\"Predicted Random Results:\")\n",
    "print_table(prediction_data, prediction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
